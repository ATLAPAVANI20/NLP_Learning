{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e35673",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_73312/3641687732.py, line 148)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\pavania\\AppData\\Local\\Temp/ipykernel_73312/3641687732.py\"\u001b[1;36m, line \u001b[1;32m148\u001b[0m\n\u001b[1;33m    1.\tCC\tCoordinating conjunction\u001b[0m\n\u001b[1;37m      \t^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Text Mining and NLP - Hands-on\n",
    "#################################\n",
    "\n",
    "sentence = \"We are Learning TextMining \"\n",
    "\n",
    "'TextMining' in sentence # verify if the text is present in the text or not\n",
    "\n",
    "sentence.index('Learning') # Check the index location\n",
    "\n",
    "sentence.split().index('TextMining') # Split the sentences into words and present the position\n",
    "\n",
    "sentence.split()[2] # 3rd word in the sentence \n",
    "\n",
    "sentence.split()[2][::-1] # Print the 3rd word in reverse order\n",
    "\n",
    "words = sentence.split() # All the words in list format\n",
    "\n",
    "first_word = words[0]\n",
    "\n",
    "last_word = words[len(words)-1] # Index in the reverse order start with -1\n",
    "\n",
    "concat_word = first_word + ' ' + last_word # join 2 words\n",
    "print(concat_word)\n",
    "\n",
    "[words[i] for i in range(len(words)) if i%2 == 0] # print the words at even index\n",
    "\n",
    "sentence[-3:] # Index in reverse starts from -1\n",
    "\n",
    "sentence[::-1] # Print entire sentence in reverse order\n",
    "\n",
    "print(' '.join([word[::-1] for word in words])) # Select each word and print it in reverse\n",
    "\n",
    "\n",
    "# Word Tokenization \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(\"I am reading NLP Fundamentals\")\n",
    "print(words)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(words) # Parts of Speech Tagging\n",
    "\n",
    "nltk.download('stopwords')  # Stop Words from nltk library\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('English') # 179 pre-defined stop words\n",
    "print(stop_words)\n",
    "\n",
    "sentence1 = \"I am learning NLP. It is one of the most popular library in Python\"\n",
    "\n",
    "sentence_words = word_tokenize(sentence1) # Tokenize the sentence\n",
    "print(sentence_words)\n",
    "\n",
    "\n",
    "\n",
    "# Stop Words\n",
    "# Filtering stop words from the input string\n",
    "sentence_no_stops = ' '.join([word for word in sentence_words if word not in stop_words]) \n",
    "print(sentence_no_stops)\n",
    "\n",
    "\n",
    "# Text Normalization\n",
    "# Replace words in string\n",
    "sentence2 = \"I visited MY from IND on 14-02-20\"\n",
    "\n",
    "normalized_sentence = sentence2.replace(\"MY\", \"Malaysia\").replace(\"IND\", \"India\").replace(\"-20\", \"-2020\")\n",
    "print(normalized_sentence)\n",
    "\n",
    "\n",
    "# Spelling Corrections\n",
    "# pip install autocorrect\n",
    "from autocorrect import Speller # Library to check typos\n",
    "spell = Speller(lang='en') # supported languages: en, pl, ru, uk, tr, es\n",
    "help(Speller)\n",
    "\n",
    "\n",
    "spell('Natureal') # Correct spelling is printed\n",
    "\n",
    "sentence3 = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")\n",
    "print(sentence3)\n",
    "\n",
    "sentence_corrected = ' '.join([spell(word) for word in sentence3])\n",
    "print(sentence_corrected)\n",
    "\n",
    "\n",
    "# Stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "stemmer.stem(\"Programming\")\n",
    "stemmer.stem(\"Programs\")\n",
    "\n",
    "stemmer.stem(\"Jumping\")\n",
    "stemmer.stem(\"Jumper\")\n",
    "\n",
    "stemmer.stem(\"battling\") # battl - stemming does not look into dictionary words\n",
    "stemmer.stem(\"amazing\")\n",
    "\n",
    "# Lemmatization\n",
    "# Lemmatization looks into dictionary words\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('Programming')\n",
    "\n",
    "lemmatizer.lemmatize('Programs')\n",
    "\n",
    "lemmatizer.lemmatize('battling')\n",
    "\n",
    "lemmatizer.lemmatize(\"amazing\")\n",
    "\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "# Chunking (Shallow Parsing) - Identifying named entities\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "sentence4 = \"We are learning nlp in Python by 360DigiTMG which is based out of India.\"\n",
    "\n",
    "i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence4)), binary=True)\n",
    "[a for a in i if len(a)==1]\n",
    "\n",
    "\n",
    "# Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"We are learning NLP in Python. Delivered by 360DigiTMG. Do you know where is it located? It is based out of India.\")\n",
    "\n",
    "\n",
    "# WSD\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "sentence1 = \"Keep your savings in the bank\"\n",
    "print(lesk(word_tokenize(sentence1), 'bank'))\n",
    "\n",
    "sentence2 = \"It's so risky to drive over the banks of the river\"\n",
    "print(lesk(word_tokenize(sentence2), 'bank'))\n",
    "\n",
    "# \"bank\" as multiple meanings. \n",
    "# The definitions for \"bank\" can be seen here:\n",
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('bank'): print(ss, ss.definition())\n",
    "\n",
    "\n",
    "#######################################\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c510283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cute']\n",
      "['cute', 'little']\n",
      "['little', 'boy']\n",
      "['boy', 'is']\n",
      "['is', 'playing']\n",
      "['playing', 'with']\n",
      "['with', 'the']\n",
      "['the', 'kitten']\n",
      "['The', 'cute', 'little']\n",
      "['cute', 'little', 'boy']\n",
      "['little', 'boy', 'is']\n",
      "['boy', 'is', 'playing']\n",
      "['is', 'playing', 'with']\n",
      "['playing', 'with', 'the']\n",
      "['with', 'the', 'kitten']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_73312/1401218186.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# pip install textblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The cute little boy is playing with the kitten.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "import re\n",
    "\n",
    "sentence5 = 'Sharat tweeted, \"Witnessing 70th Republic Day of India from Rajpath, \\\n",
    "New Delhi. Mesmerizing performance by Indian Army! Awesome airshow! @india_official \\\n",
    "@indian_army #India #70thRepublic_Day. For more photos ping me sharat@photoking.com :)\"'\n",
    "\n",
    "sentence5.split()\n",
    "re.sub(r'([^\\s\\w]|_)+', ' ', sentence5).split()\n",
    "\n",
    "\n",
    "# Extracting n-grams\n",
    "# n-grams can be extracted from 3 different techniques:\n",
    "# listed below are:\n",
    "# 1. Custom defined function\n",
    "# 2. NLTK\n",
    "# 3. TextBlob\n",
    "\n",
    "# Extracting n-grams using customed defined function\n",
    "import re\n",
    "def n_gram_extractor(input_str, n):\n",
    "    tokens = re.sub(r'([^\\s\\w]|_)+', ' ', input_str).split()\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        print(tokens[i:i+n])\n",
    "\n",
    "n_gram_extractor('The cute little boy is playing with the kitten.', 2)\n",
    "\n",
    "n_gram_extractor('The cute little boy is playing with the kitten.', 3)\n",
    "\n",
    "\n",
    "# Extracting n-grams with nltk\n",
    "from nltk import ngrams\n",
    "list(ngrams('The cute little boy is playing with the kitten.'.split(), 2))\n",
    "\n",
    "list(ngrams('The cute little boy is playing with the kitten.'.split(), 3))\n",
    "\n",
    "\n",
    "# Extracting n-grams using TextBlob\n",
    "# TextBlob is a Python library for processing textual data.\n",
    "\n",
    "# pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"The cute little boy is playing with the kitten.\")\n",
    "\n",
    "blob.ngrams(n=2)\n",
    "\n",
    "blob.ngrams(n=3)\n",
    "\n",
    "\n",
    "# Tokenizing texts with different packages: Keras, Textblob\n",
    "sentence5 = 'Sharat tweeted, \"Witnessing 70th Republic Day of India from Rajpath, New Delhi. Mesmerizing performance by Indian Army! Awesome airshow! @india_official @indian_army #India #70thRepublic_Day. For more photos ping me sharat@photoking.com :)\"'\n",
    "\n",
    "# pip install tensorflow\n",
    "# pip install keras\n",
    "\n",
    "# Tokenization with Keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text_to_word_sequence(sentence5)\n",
    "\n",
    "# Tokenization with TextBlob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(sentence5)\n",
    "blob.words\n",
    "\n",
    "# Tokenize sentences using other nltk tokenizers:\n",
    "# 1. Tweet Tokenizer\n",
    "# 2. MWE Tokenizer (Multi-Word Expression)\n",
    "# 3. Regexp Tokenizer\n",
    "# 4. Whitespace Tokenizer\n",
    "# 5. Word Punct Tokenizer\n",
    "\n",
    "\n",
    "# 1. Tweet tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokenizer.tokenize(sentence5)\n",
    "\n",
    "# 2. MWE Tokenizer (Multi-Word Expression)\n",
    "from nltk.tokenize import MWETokenizer\n",
    "mwe_tokenizer = MWETokenizer([('Republic', 'Day')]) # Declaring set of words that are to be treated as one entity\n",
    "mwe_tokenizer.add_mwe(('Indian', 'Army')) # Adding more words to the set\n",
    "\n",
    "mwe_tokenizer.tokenize(sentence5.split()) #  Indian Army' should be treated as a single token. But here \"Army!\" is treated as a token. \n",
    "\n",
    "mwe_tokenizer.tokenize(sentence5.replace('!', '').split()) # \"Army!\" will be treated as Army \n",
    "\n",
    "\n",
    "# 3. Regexp Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "reg_tokenizer.tokenize(sentence5)\n",
    "\n",
    "\n",
    "# 4. Whitespace Tokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wh_tokenizer = WhitespaceTokenizer()\n",
    "wh_tokenizer.tokenize(sentence5)\n",
    "\n",
    "\n",
    "# 5. WordPunct Tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "wp_tokenizer = WordPunctTokenizer()\n",
    "wp_tokenizer.tokenize(sentence5)\n",
    "\n",
    "\n",
    "# Stemming\n",
    "# Regexp Stemmer\n",
    "sentence6 = \"I love playing Cricket. Cricket players practice hard in their innings .\"\n",
    "from nltk.stem import RegexpStemmer\n",
    "regex_stemmer = RegexpStemmer('ing$')\n",
    "\n",
    "' '.join([regex_stemmer.stem(wd) for wd in sentence6.split()])\n",
    "\n",
    "\n",
    "# Porter Stemmer\n",
    "sentence7 = \"Before eating, it would be nice to sanitize your hands with a sanitizer\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps_stemmer = PorterStemmer()\n",
    "' '.join([ps_stemmer.stem(wd) for wd in sentence7.split()])\n",
    "\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence8 = \"The codes executed today are far better than what we execute generally.\"\n",
    "\n",
    "' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(sentence8)])\n",
    "\n",
    "\n",
    "# Singularize & Pluralize words\n",
    "from textblob import TextBlob\n",
    "sentence9 = TextBlob('She sells seashells on the seashore')\n",
    "sentence9.words\n",
    "\n",
    "sentence9.words[2].singularize()\n",
    "\n",
    "sentence9.words[5].pluralize()\n",
    "\n",
    "\n",
    "# Language Translation\n",
    "# From Spanish to English\n",
    "\n",
    "from textblob import TextBlob\n",
    "en_blob = TextBlob(u'muy bien')\n",
    "en_blob.translate(from_lang='es', to='en') \n",
    "\n",
    "\n",
    "# Custom Stop words removal\n",
    "from nltk import word_tokenize\n",
    "sentence9 = \"She sells seashells on the seashore\"\n",
    "custom_stop_word_list = ['she', 'on', 'the', 'am', 'is', 'not']\n",
    "' '.join([word for word in word_tokenize(sentence9) if word.lower() not in custom_stop_word_list])\n",
    "\n",
    "\n",
    "# Extracting general features from raw texts\n",
    "\n",
    "# Number of words\n",
    "# Detect presence of wh words\n",
    "# Polarity\n",
    "# Subjectivity\n",
    "# Language identification\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([['The vaccine for covid-19 will be announced on 1st August.'],\n",
    "                   ['Do you know how much expectation the world population is having from this research?'],\n",
    "                   ['This risk of virus will end on 31st July.']])\n",
    "df.columns = ['text']\n",
    "df\n",
    "\n",
    "# Number of words\n",
    "from textblob import TextBlob\n",
    "df['number_of_words'] = df['text'].apply(lambda x : len(TextBlob(x).words))\n",
    "df['number_of_words']\n",
    "\n",
    "# Detect presence of wh words\n",
    "wh_words = set(['why', 'who', 'which', 'what', 'where', 'when', 'how'])\n",
    "df['is_wh_words_present'] = df['text'].apply(lambda x : True if len(set(TextBlob(str(x)).words).intersection(wh_words)) > 0 else False)\n",
    "df['is_wh_words_present']\n",
    "\n",
    "\n",
    "# Polarity\n",
    "df['polarity'] = df['text'].apply(lambda x : TextBlob(str(x)).sentiment.polarity)\n",
    "df['polarity']\n",
    "\n",
    "# Subjectivity\n",
    "df['subjectivity'] = df['text'].apply(lambda x : TextBlob(str(x)).sentiment.subjectivity)\n",
    "df['subjectivity']\n",
    "\n",
    "# Language of the sentence\n",
    "df['language'] = df['text'].apply(lambda x : TextBlob(str(x)).detect_language())\n",
    "df['language']\n",
    "\n",
    "\n",
    "# Bag of Words\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['At least seven Indian pharma companies are working to develop a vaccine against coronavirus',\n",
    "'the deadly virus that has already infected more than 14 million globally.',\n",
    "'Bharat Biotech, Indian Immunologicals, are among the domestic pharma firms working on the coronavirus vaccines in India.'\n",
    "]\n",
    "\n",
    "bag_of_words_model = CountVectorizer()\n",
    "print(bag_of_words_model.fit_transform(corpus).todense()) # bag of words\n",
    "\n",
    "bag_of_word_df = pd.DataFrame(bag_of_words_model.fit_transform(corpus).todense())\n",
    "bag_of_word_df.columns = sorted(bag_of_words_model.vocabulary_)\n",
    "bag_of_word_df.head()\n",
    "\n",
    "# Bag of word model for top 5 frequent terms\n",
    "bag_of_words_model_small = CountVectorizer(max_features=5)\n",
    "bag_of_word_df_small = pd.DataFrame(bag_of_words_model_small.fit_transform(corpus).todense())\n",
    "bag_of_word_df_small.columns = sorted(bag_of_words_model_small.vocabulary_)\n",
    "bag_of_word_df_small.head()\n",
    "\n",
    "# TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "print(tfidf_model.fit_transform(corpus).todense())\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(corpus).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()\n",
    "\n",
    "# TFIDF for top 5 frequent terms\n",
    "tfidf_model_small = TfidfVectorizer(max_features=5)\n",
    "tfidf_df_small = pd.DataFrame(tfidf_model_small.fit_transform(corpus).todense())\n",
    "tfidf_df_small.columns = sorted(tfidf_model_small.vocabulary_)\n",
    "tfidf_df_small.head()\n",
    "\n",
    "\n",
    "# Feature Engineering (Text Similarity)\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pair1 = [\"Do you have Covid-19\",\"Your body temperature will tell you\"]\n",
    "pair2 = [\"I travelled to Malaysia.\", \"Where did you travel?\"]\n",
    "pair3 = [\"He is a programmer\", \"Is he not a programmer?\"]\n",
    "\n",
    "def extract_text_similarity_jaccard (text1, text2):\n",
    "    words_text1 = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text1)]\n",
    "    words_text2 = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text2)]\n",
    "    nr = len(set(words_text1).intersection(set(words_text2)))\n",
    "    dr = len(set(words_text1).union(set(words_text2)))\n",
    "    jaccard_sim = nr/dr\n",
    "    return jaccard_sim\n",
    "\n",
    "extract_text_similarity_jaccard(pair1[0], pair1[1])\n",
    "extract_text_similarity_jaccard(pair2[0], pair2[1])\n",
    "extract_text_similarity_jaccard(pair3[0], pair3[1])\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "\n",
    "# Creating a corpus which will have texts of pair1, pair2 and pair 3 respectively\n",
    "corpus = [pair1[0], pair1[1], pair2[0], pair2[1], pair3[0], pair3[1]]\n",
    "\n",
    "tfidf_results = tfidf_model.fit_transform(corpus).todense()\n",
    "# Note: Here tfidf_results will have tf-idf representation of \n",
    "# texts of pair1, pair2 and pair3 in the given order.\n",
    "\n",
    "# tfidf_results[0], tfidf_results[1] represents pair1\n",
    "# tfidf_results[2], tfidf_results[3] represents pair2\n",
    "# tfidf_results[4], tfidf_results[5] represents pair3\n",
    "\n",
    "#cosine similarity between texts of pair1\n",
    "cosine_similarity(tfidf_results[0], tfidf_results[1])\n",
    "\n",
    "#cosine similarity between texts of pair2\n",
    "cosine_similarity(tfidf_results[2], tfidf_results[3])\n",
    "\n",
    "#cosine similarity between texts of pair3\n",
    "cosine_similarity(tfidf_results[4], tfidf_results[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # Importing requests to extract content from a url\n",
    "from bs4 import BeautifulSoup as bs # Beautifulsoup is for web scrapping...used to scrap specific content \n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# creating empty reviews list\n",
    "oneplus_reviews=[]\n",
    "\n",
    "for i in range(1,21):\n",
    "  ip=[]  \n",
    "  url=\"https://www.amazon.in/OnePlus-Silver-Storage-hands-free-capable/product-reviews/B09MQBRCSZ/ref=cm_cr_getr_d_paging_btm_prev_1?ie=UTF8&reviewerType=all_reviews&pageNumber=\"+str(i)  \n",
    "  response = requests.get(url)\n",
    "  soup = bs(response.content,\"html.parser\")# creating soup object to iterate over the extracted content \n",
    "  reviews = soup.find_all(\"span\", attrs={\"class\",\"a-size-base review-text review-text-content\"})# Extracting the content under specific tags  \n",
    "  for i in range(len(reviews)):\n",
    "    ip.append(reviews[i].text)  \n",
    " \n",
    "  oneplus_reviews = oneplus_reviews + ip  # adding the reviews of one page to empty list which in future contains all the reviews\n",
    "\n",
    "# writng reviews in a text file \n",
    "with open(\"oneplus.txt\", \"w\", encoding='utf8') as output:\n",
    "    output.write(str(oneplus_reviews))\n",
    "\t\n",
    "\n",
    "# Joinining all the reviews into single paragraph \n",
    "ip_rev_string = \" \".join(oneplus_reviews)\n",
    "\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# Removing unwanted symbols incase if exists\n",
    "ip_rev_string = re.sub(\"[^A-Za-z\" \"]+\", \" \", ip_rev_string).lower()\n",
    "# ip_rev_string = re.sub(\"[0-9\" \"]+\",\" \", ip_rev_string)\n",
    "\n",
    "# words that contained in the reviews\n",
    "ip_reviews_words = ip_rev_string.split(\" \")\n",
    "\n",
    "ip_reviews_words = ip_reviews_words[1:]\n",
    "\n",
    "#TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(ip_reviews_words)\n",
    "\n",
    "\n",
    "with open(\"C:\\\\Data\\\\textmining\\\\stop.txt\", \"r\") as sw:\n",
    "    stop_words = sw.read()\n",
    "    \n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "\n",
    "stop_words.extend([\"oneplus\",\"mobile\",\"time\",\"android\",\"phone\",\"device\",\"product\",\"day\"])\n",
    "\n",
    "ip_reviews_words = [w for w in ip_reviews_words if not w in stop_words]\n",
    "\n",
    "# Joinining all the reviews into single paragraph \n",
    "ip_rev_string = \" \".join(ip_reviews_words)\n",
    "\n",
    "# WordCloud can be performed on the string inputs.\n",
    "# Corpus level word cloud\n",
    "\n",
    "wordcloud_ip = WordCloud(background_color='White',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(ip_rev_string)\n",
    "plt.imshow(wordcloud_ip)\n",
    "\n",
    "# positive words # Choose the path for +ve words stored in system\n",
    "with open(\"C:\\\\Data\\\\textmining\\\\positive-words.txt\", \"r\") as pos:\n",
    "  poswords = pos.read().split(\"\\n\")\n",
    "\n",
    "# Positive word cloud\n",
    "# Choosing the only words which are present in positive words\n",
    "ip_pos_in_pos = \" \".join ([w for w in ip_reviews_words if w in poswords])\n",
    "\n",
    "wordcloud_pos_in_pos = WordCloud(\n",
    "                      background_color='White',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(ip_pos_in_pos)\n",
    "plt.figure(2)\n",
    "plt.imshow(wordcloud_pos_in_pos)\n",
    "\n",
    "# negative words Choose path for -ve words stored in system\n",
    "with open(\"C:\\\\Data\\\\textmining\\\\negative-words.txt\", \"r\") as neg:\n",
    "  negwords = neg.read().split(\"\\n\")\n",
    "\n",
    "# negative word cloud\n",
    "# Choosing the only words which are present in negwords\n",
    "ip_neg_in_neg = \" \".join ([w for w in ip_reviews_words if w in negwords])\n",
    "\n",
    "wordcloud_neg_in_neg = WordCloud(\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(ip_neg_in_neg)\n",
    "plt.figure(3)\n",
    "plt.imshow(wordcloud_neg_in_neg)\n",
    "\n",
    "#################################################################\n",
    "# Joinining all the reviews into single paragraph \n",
    "ip_rev_string = \" \".join(oneplus_reviews)\n",
    "\n",
    "# wordcloud with bigram\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "WNL = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Lowercase and tokenize\n",
    "text = ip_rev_string.lower()\n",
    "\n",
    "# Remove single quote early since it causes problems with the tokenizer.\n",
    "text = text.replace(\"'\", \"\")\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "text1 = nltk.Text(tokens)\n",
    "\n",
    "# Remove extra chars and remove stop words.\n",
    "text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text1]\n",
    "\n",
    "# Create a set of stopwords\n",
    "stopwords_wc = set(STOPWORDS)\n",
    "customised_words = ['price', 'great', '9rt'] # If you want to remove any particular word form text which does not contribute much in meaning\n",
    "\n",
    "new_stopwords = stopwords_wc.union(customised_words)\n",
    "\n",
    "# Remove stop words\n",
    "text_content = [word for word in text_content if word not in new_stopwords]\n",
    "\n",
    "# Take only non-empty entries\n",
    "text_content = [s for s in text_content if len(s) != 0]\n",
    "\n",
    "# Best to get the lemmas of each word to reduce the number of similar words\n",
    "text_content = [WNL.lemmatize(t) for t in text_content]\n",
    "\n",
    "# nltk_tokens = nltk.word_tokenize(text)  \n",
    "bigrams_list = list(nltk.bigrams(text_content))\n",
    "print(bigrams_list)\n",
    "\n",
    "dictionary2 = [' '.join(tup) for tup in bigrams_list]\n",
    "print (dictionary2)\n",
    "\n",
    "# Using count vectoriser to view the frequency of bigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "bag_of_words = vectorizer.fit_transform(dictionary2)\n",
    "vectorizer.vocabulary_\n",
    "\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "print(words_freq[:100])\n",
    "\n",
    "# Generating wordcloud\n",
    "words_dict = dict(words_freq)\n",
    "WC_height = 1000\n",
    "WC_width = 1500\n",
    "WC_max_words = 100\n",
    "wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width, stopwords=new_stopwords)\n",
    "\n",
    "wordCloud.generate_from_frequencies(words_dict)\n",
    "plt.figure(4)\n",
    "plt.title('Most frequently occurring bigrams connected by same colour and font size')\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4104f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
